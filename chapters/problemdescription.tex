%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{MBT in agilen Entwicklungsumgebungen}
\label{sec:problemdescription}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Fallstudie: Kundenberaterapplikation bei Raiffeisen Schweiz}

\subsection{Hintergrund zu den Applikation RETo und RESi}
Die Applikation \textit{RESi}, in anderer Struktur und bis in das Jahr 2013 \textit{RETo} (Raiffeisen Expert Tool), ist ein Backend-Service der Dienste für die verschiedensten Front-End Applikationen des Unternehmens bereitstellt. Unter altem Namen, bot die Applikation früher eine eigene Benutzeroberfläche und weniger, dafür spezifischere Funktionalität. RETo war im Intranet der Fillialen im Einsatz und wurde ausschließlich für Kundenberatungen eingesetzt. Die Applikation hatte folgende Kernkompetenzen:

\begin{itemize}
\item Ermittlung von Anlegerprofilen
\item Durchführung von Anlage-Checks
\item Beratungen zum Thema Wohnen und Wohneigentum
\item Beratungen zum Thema Vorsorge
\item Beratungen zum Thema Pension
\end{itemize}

Im Jahr 2014 wurde ein Großteil der Applikationslandschaft umstrukturiert. In einer Bemühung einzelne Applikationen zu vereinheitlichen und Mehrfachaufwendungen zu minimieren, wurde RETo modularisiert. Das Projekt RESi wurde gestartet. RESi stellt eine Back-End-Service Schicht dar. Da RETo bereits einen sehr breiten Funktionsumfang bot, besteht RESi zu großen Teilen aus den RETo Kernkomponenten. Auch das Entwicklungsteam ist das selbe geblieben. Mehrere, ehemals eigenständige Applikationen, greifen jetzt auf RESi zu und bieten nur noch ein Front-End und eine dünne Serverschicht. RETo existiert weiterhin aber greift auch auf die Services von RESi zu.

\subsection{Entwicklungsumfeld}
Das Umfeld er Applikaiton RESi wird in einem klassischen Auftragnehmer/Auftraggeber Szenario entwickelt. Eine Fachabteilung stellt den Kunden dar. Sie definieren die Anforderungen an die Applikation. Dem gegenüber ist das Entwicklungsteam. Dieses besteht aus einem Applikationsmanager und 6-8 Entwicklern. 

\begin{itemize}
\item 2 Major Release pro Jahr
\item Mehrere Service-Release pro Jahr
\item 3-wöchige Scrum-Sprints
\item 4 Ebenen (Integrationstest, Systemtest, Akzeptanztest, Produktion)
\item Scrum Master wechselt
\item Product Owner (PO) wird von Fachbereich gestellt
\item tägliche Scrum-Meetings
\end{itemize} 

Die Applikation RESi ist, bedingt durch ihre Reife, bereits sehr stabil. Effektiv ist sie schon seit mehreren Jahren (in anderer Struktur und unter anderem Namen) im produktiven Einsatz. Der typische Entwicklungszyklus wird also vom Fachbereich angestoßen. Ein Request For Change (RFC) wird vom Fachbereich angenommen oder verfasst. Die zuständigen Produktmanager definieren genügend fachliche Details, bevor ein RFC in eine Story verwandelt wird. Diese Story fließt nun typischerweise in den Scrum-Backlog (\todo{Ref zu Scrum Kapitel}). Wenn auf einer der Entwicklungsebenen Fehler mit hoher Priorität gefunden werden, wird ein Defect-Bericht verfasst, der direkt in den laufenden Entwicklungszyklus (Sprint) einfließt.\\
Auf Seiten der Entwickler finden kurze tägliche Meetings statt. Dauer und Struktur dieser täglichen Meetings entsprechen den klassischen Daily-Scrum Standups. Neben den Entwicklern ist der Scrum-Master und der Product Owner aus dem Fachbereich anwesend. Er steht für kurzfristig auftretende Fragen bereit. Weiters finden im Entwicklungsteam wöchentliche Research-Meetings statt. In diesen werden die neu eingetroffenen Stories analysiert und modularisiert. Ziel ist es, eine Story in Tasks herunterzubrechen, die in einem Arbeitstag schaffbar sind. Ein Task soll also von einem einzelnen Entwickler implementiert werden. Im selben Zug, wird werden die Aufwände der Story und damit der Tasks geschätzt. Im RESi Team kommen zwei verschiedene Methoden zur Aufwandsschätzung zum Einsatz. Einerseits wird eine Methode verwendet wo alle Stories offen und ungeordnet aufgelegt werden. Nun wird das Team der Reihe nach gebeten, eine Story einzuordnen. Damit werden weder Storypoints noch Stunden geschätzt. Stories werden anhand ihrer augenscheinlichen Aufwänden geordnet. Wenn kein Teammitglied mehr eine Änderung machen will, endet die Aufwandsschätzung. Der Scrum-Master legt schlussendlich, in Absprache mit dem Team, fest welcher Bereich von Stories einen zukünftigen Sprint fließen. Nimmt man Stories aus dem vorderen Bereich der Reihung, mindert dies den Gesamtaufwand viel höher als wenn Stories aus dem hinteren Teil zurück in den Backlog verschoben werden. 

\subsection{Entwicklungsebenen}
Zwischen der Implementierung eines Tasks und dessen Eintritt in eine produktive Umgebung, läuft dieser durch definierte Ebenen. Während der Bearbeitung eines Tasks, benutzen die Entwickler eine lokale Installation der Applikation (diese entspricht der Ebene \textit{Integrationstest}). Wenn ein Entwickler einen Task abschließt und alle Unit-Tests erwartungsgemäß durchlaufen werden, wird der Task zum \textit{Code Review} freigegeben. Ein anderer Entwickler liest den Code und gibt Feedback zu Richtigkeit, Effizienz, Lesbarkeit und Stil des Code-Stücks. Das Code-Stück wird, nach eventueller Korrektur, auf der Ebene \textit{Integrationstest} deployed (ein IBM WebSphere Applikationsserver \footnote{IBM WebSphere \url{http://www.ibm.com/websphere}} im Falle von RESi). Diese Ebene hat die Hauptaufgabe, auftretende Nebeneffekte aufzudecken, die das neu programmierte Code-Stück verursacht. Ab diesem Zeitpunkt beginnt der Fachbereich bereits mit manuellen Tests auf dem Integrationsserver. In manchen Fällen macht es keinen Sinn gegen einen einzelnen Task zu testen (möglicherweise lässt er sich GUI-seitig auch gar nicht testen). Dann wird abgewartet bis sich verwandte Tasks oder die ganze zugehörige Story für den Integrationsserver freigegeben werden. Da Entwickler und Fachbereich täglich auf dieser Ebene arbeiten, werden auftretende Nebeneffekte durch Wartungsänderungen eher entdeckt.\\
Nach Abschluss eines Sprints wird der Stand der Ebene \textit{Integrationstest} auf \textit{Systemtest} deployed. Hier testet der Fachbereich genau definierte Abläufe. Außerdem unternimmt eine gesonderte Test-Abteilung Last- und Performance Tests auf dieser Ebene. Bis zu diesem Zeitpunkt läuft die Entwicklung relativ streng nach agilen Prinzipien. Um das Zusammenspiel der Applikationslandschaft zu vereinheitlichen, wird unternehmensweit aber immer noch auf viel längere Release-Zyklen gesetzt. Sprint-Ergebnisse werden also nicht zeitnah in die Produktionsebene versetzt. Die Ebene \textit{Akzeptanztest} wird also zwischen Systemtest und \textit{Produktionsebene} gezogen. Auf ihr werden die Stände getestet, die für Major-Releases geplant sind. Typischerweise wird gegen Ende eines Major-Release Zyklus verstärkt auf der Ebene \textit{Akzeptanztest} deployed und getestet. Trotzdem kann es zu Überlappungen kommen die mit dem agilen Iterationszyklus interferieren. Sprint-Ergebnisse die kurz vor einem Major-Release eigentlich für produktionsreife getestet werden sollten, werden möglicherweise erst für die nächste Veröffentlichung beachtet. Erstens werden also Testressourcen periodisch für \textit{Akzeptanztest} benötigt, obwohl bereits neuere Versionen der Applikation testbar wären. Zweitens leidet das Endprodukt wenn Features, die eigentlich zur Veröffentlichung freigegeben werden könnten, mehrmonatige Verspätungen haben.

\subsection{Qualitätssicherung im Projekt}
\subsubsection{Versuch der Qualitätssicherung durch skriptgesteuerte GUI-Tests}
\subsubsection{Aufwendige Wartung von Testdaten}
Tracing von Testdaten und Testfällen (welcher Bereich der Testdaten wird von einem bestimmten Testfall benutzt?). `If data is tightly coupled with behaviour, this can lead to serious maintenance burden' \cite{baker_model-driven_2005} 
\subsubsection{Abhilfe durch Modellbasiertes Testen der Schnittstellen?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{MBT auf Unit-Testebene}
\subsection{ModelJUnit}
ModelJUnit wird von Dr. Mark Utting et al. an der Universität Waikato in Neuseeland entwickelt. Es wird unter der Open Source GNU GPL Lizenz entwickelt und befindet sich zur Zeit in der Version 2.5 \footnote{Homepage des Tools ModelJUnit \url{http://www.cs.waikato.ac.nz/~marku/mbt/modeljunit/}}. Es erlaubt die Modellierung des SUT als endlicher Automat (von hier an FST für \textit{finite state machine}) in Java, Generierung von Testfällen laut spezifiziertem Traversierungsalgorithmus sowie Reporting. Seit Version 2.0 bietet das Tool eine grafische Benutzeroberfläche die die Traversierungskonfiguration ermöglicht und veranschaulicht.\\

\subsection{Endliche Automaten - Finite State Machines}
Ein endlicher Automat (oder \textit{finite state machine}) ist eine Form der abstrakten Maschinen. Eine FSM erlaubt die Modellierung des Verhaltens einer Maschine, in diesem Fall eines Computer-Programms.\\


\subsubsection{Traversierung}
\paragraph{Random Walk}
\paragraph{Greedy Walk}
\paragraph{Lookahead Walk}
\paragraph{Quick Walk}


\section{MBT auf Integrationstestebene}
\subsection{Testing von Serviceorientierten Architekturen mittels UML Testing Profile}

\subsubsection{UML Testing Profile BASICS}

\subsubsection{SOA und Web-Services}
Von Serviceorientierten Architekturen (von hier an \textit{SOA}) verspricht man sich schnelle und einfache Integration innerhalb als auch über Unternehmensgrenzen hinweg. Die Möglichkeit eine Applikation mittels Komposition, maßgeschneidert zu den gegebenen Requirements, zusammenzustellen, ist traditionellen Software-Engineering Methoden oft voraus. Gleichzeitig stellen verschachtelte und unabhängige Strukturen den Tester vor neue Herausforderungen. \\
SOA wird missbräuchlich oft mit Web-Services gleichgestellt. Tatsächlich sind Web-Services nur die häufigste architektonische Grundlage auf der SOA-Applikationen laufen. Die folgende Sektion geht ebenfalls von einer Serviceorientierten Architektur basierend auf Web-Services aus.

\subsubsection{Mapping von Web-Service Elementen auf UML-Diagramme}
Am Beispiel eines Web-Services der per WSDL\footnote{WSDL Web-Service Spezifikationssprache 2.0 \url{http://www.w3.org/TR/wsdl20/}} definiert ist, soll gezeigt werden wie eine Modellierung von Web-Services mittels dem UML-Testing Profile umgesetzt werden kann. \footnote{Sehr ähnlich würde eine Modellierung für RESTful Web-Services basierend auf WADL ausschauen. Hierbei stellt sich aber eine Grundsatzfrage: Eines der Prinzipien von REST ist die Fähigkeit, dass sich ein Web-Service selbst beschreiben kann. Inwiefern trotzdem WADL Dateien gebraucht werden, hängt von der jeweiligen Implementierung ab.} Baker et al. schlagen folgende Vorgehensweise für das Mapping vor\cite{_model-driven_2007}:

\begin{itemize}
\item WSDL Port Types werden zu UML Stereotyp-Klassen gemappt
\item Die Operationen in dieser Klasse stellen die Operationen dar, die der jeweilige Port Type offenbart.
\item Jede Operation hat eine übereinstimmende Request Message. Falls die Operation auch einen Rückgabewert definiert, wird auch dieser abgebildet.
\item Komplexe Typen als auch Enumerationen werden zu stereotypisierten Klassen.
\end{itemize}

\subsubsection{Teststrategie für Web Services}
Im Gegensatz zu herkömmlichen Desktop-Applikationen aus einer Hand, können SOA Web-Services tief verschachtelte Komponenten von verschiedensten Parteien enthalten. Dies erschwert das Testing auf zwei Arten. Erstens können gravierende Qualitätsunterschiede zwischen Web-Services bestehen. Zweitens unterliegen diese benutzten Web-Services eigenen Wartungs- und Änderungsintervallen. Diese Erkenntnisse haben zur Folge, dass an die Teststrategie für Web-Services folgende zusätzliche Anforderungen gestellt werden:

\begin{itemize}
\item Tests müssen schnell und oft durchführbar sein (nämlich dann wenn eine Komponente geändert wird): \textit{On-Demand Testing}.
\item Gleichzeitig müssen die Tests kompakt und schnell wartbar sein (ähnlich Unit-Tests).
\item Web-Services sollen einzeln getestet werden können. Dies ermöglicht nicht nur eine gewisse Modularität, die zu einer hohen Test-Geschwindigkeit beiträgt sondern erleichtert auch die Fehlersuche einem negativen Testdurchlauf.
\end{itemize}

Web-Services müssen \textit{mehrdimensional} getestet werden\cite{_model-driven_2007}. Das bedeutet, ein modellbasierter Testfall soll alle Port Types, die der Service zur Verfügung stellt, kombiniert mit allen Operationen die jeder Port Type anbietet, prüfen. Als Testdaten müssen die Equivalenzklassen aller Datentypen die der Web-Service anbietet identifiziert werden.

\subsubsection{Beispielhafte TestSuite eines SOA Web-Services}
\todo{UML-Diagramme (und evt. WSDL)aus Buch einbinden} In diesem Beispiel wird ein Service einer Bücherei oder einer Buchhandlung herangezogen. Einfachheitshalber bietet dieser Service nur einen Port Type (LibraryService) an. Auf diesem werden drei Operationen angeboten (search, reserve, fetch). Ein Client kann ein Element aus dieser Bücherei also suchen und basierend auf seinem Status-Parameter handeln. Das Element kann sofort verfügbar, später verfügbar und nicht lokal verfügbar sein.\\

Um die angesprochene mehrdimensionale Abdeckung zu gewährleisten bietet das UML Testing Profile das Konzept der Data Pools.\todo{Ref zu Kapitel UTP}. Der DataPool stellt das kartesische Produkt dar, dass aus Operationen und Datenelementen gebildet wird. Nun kann ein Testtreiber auf diesem DataPool operieren und auf Testfalldiagramme (UML-Sequenzdiagramme) referenzieren.\\
Anhand von diesem einfachen Beispiel wird sichtbar, dass das UML Testing Profile sinnvolle Erweiterungen definiert, die die Modellierung von modernen SOA-Applikationen vereinfachen. Data Pools visualisieren die Abdeckung von Äquivalenzklassen und modularisierte Sequenzdiagramme erlauben die Modellierung von umfangreichen Testfällen. 


\subsubsection{Ausfühhrung von UTP Tests mittels JUnit}

\subsection{Schnittstellentests mit Graphwalker}
\subsection{Einbindung SoapUI/HP UFT}


\section{MBT auf Systemtestebene und Test non-stop}
\subsection{Manuelle Systemtests anhand von Modellen}
\subsection{Automatisierte MBT GUI-Tests (GUITAR)}
\subsection{Automatische Modellgenerierung auf Produktionsebene (LearnLib)}
